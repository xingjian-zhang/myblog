---
title: Learning with Local and Global Consistency 
author: Xingjian Zhang
date: 2023-03-23
category: classics
layout: post
---

Keywords: Graph, Semi-Supervised Learning, Graph Smoothing, Transductive
Inference, Regularization.

## Summary

Just like the [previous blog]({% post_url 2023-03-21-graph %}), this paper is
about semi-supervised learning from labeled (rare) and unlabeled (common) data.
My takeaway from this paper is that the key of semi-supervised learning is the
prior assumption of **consistency**, which means
1. (local consistency) nearby points in the data space are likely to have
   similar labels, and
2. (global consistency) points on the same structure (e.g. a cluster or a
   manifold) are likely to have similar labels. This assumption is often called
**cluster assumption** in the literature.

Another common point between the two papers is that they both transform the
Euclidean space into a discrete graph space (which means the corresponding
graph is always dense), and then use graph smoothing to constrain the label
assignment.

## Problem Formulation

Given a point set $\mathcal{X}=\left\\{x_1, \ldots, x_l, x_{l+1}, \ldots,
x_n\right\\} \subset \mathbb{R}^m$ and a label set $\mathcal{L}=\{1, \ldots,
c\}$ the first $l$ points $x_i(i \leq l)$ are labeled as $y_i \in \mathcal{L}$
and the remaining points $x_u(l+1 \leq u \leq n)$ are unlabeled. The goal is to
predict the label of the unlabeled points.


Let $\mathcal{F}$ denote the set of $n \times c$ matrices with nonnegative
entries. A matrix $F=$ $\left[F_1^T, \ldots, F_n^T\right]^T \in \mathcal{F}$
corresponds to a classification on the dataset $\mathcal{X}$ by labeling each
point $x_i$ as a label $y_i=\arg \max_{j \leq c} F_{i j}$. We can understand
$F$ as a vectorial function $F: \mathcal{X} \rightarrow \mathbb{R}^c$ which
assigns a vector $F_i$ to each point $x_i$. Define a $n \times c$ matrix $Y \in
\mathcal{F}$ with $Y_{i j}=1$ if $x_i$ is labeled as $y_i=j$ and $Y_{i j}=0$
otherwise.

## Algorithm

The authors develop a iterative algorithm and then prove that it converges to a
closed-form solution. Additionally, the algorithm is essentially related to a
graph or diffusion kernel. The intuition of this algorithm is that the given
labels are diffused to the unlabeled points by a diffusion kernel. 

1. Form the affinity matrix $W$ defined by $W_{i j}=\exp
   \left(-\left\\|x_i-x_j\right\\|^2 / 2 \sigma^2\right)$ if $i \neq j$ and
   $W_{i i}=0$.
2. Construct the matrix $S=D^{-1 / 2} W D^{-1 / 2}$ in which $D$ is a diagonal
   matrix with its $(i, i)$-element equal to the sum of the $i$-th row of $W$.
3. Iterate $F(t+1)=\alpha S F(t)+(1-\alpha) Y$ until convergence, where
   $\alpha$ is a parameter in $(0,1)$.
4. Let $F^\ast$ denote the limit of the sequence $\\{F(t)\\}$. Label each point
   $x_i$ as a label $y_i=\arg \max_{j \leq c} F_{i j}^*$

> Notice that the $S$ matrix is the [normalized adjacency matrix](https://people.orie.cornell.edu/dpw/orie6334/Fall2016/lecture7.pdf) of the graph.
{: .block-tip }

It is not hard to show

\[ F^*=\lim _{t \rightarrow \infty} F(t)=(1-\alpha)(I-\alpha S)^{-1} Y \]

## Regularization framework

The author also shows the above algorithm is equivalent to optimizing the
following regularized objective function: \[
\mathcal{Q}(F)=\frac{1}{2}\left(\sum_{i, j=1}^n W_{i
j}\left\\|\frac{1}{\sqrt{D_{i i}}} F_i-\frac{1}{\sqrt{D_{j j}}}
F_j\right\\|^2+\mu \sum_{i=1}^n\left\\|F_i-Y_i\right\\|^2\right) \]

The first term of the right-hand side in the cost function is the smoothness
constraint, which means that a good classifying function should not change too
much between nearby points. The second term is the fitting constraint, which
means a good classifying function should not change too much from the initial
label assignment. The trade-off between these two competing constraints is
captured by a positive parameter: Note that the fitting constraint contains
labeled as well as unlabeled data.

## References
*Dengyong Zhou, Olivier Bousquet, Thomas Navin Lal, Jason Weston, and Bernhard Schölkopf. 2003. Learning with local and global consistency. In Proceedings of the 16th International Conference on Neural Information Processing Systems (NIPS'03). MIT Press, Cambridge, MA, USA, 321–328.*