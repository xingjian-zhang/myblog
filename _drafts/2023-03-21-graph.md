---
title: Semi-Supervised Learning Using Gaussian Fields and Harmonic Functions
author: Xingjian Zhang
date: 2023-03-21
category: Jeykll
layout: post
---

_This is the very first blog I have written in 2023. This collection of blogs
are all about famous literature (classics) in graph learning, causal inference,
and data science. By writing these blogs, my aim is to learn and collect
insights that are often overlooked in our study of "neural nets-based ML" and
to rethink the potential of traditional statistical models and methods._

Keywords: Gaussian Fields, Harmonic Functions, Semi-Supervised Learning, Graph
Smoothing.

## Summary

This paper is about the task to perform semi-supervised learning on graphs. In
specific, labelled and unlabelled data are represented as nodes in a weighted
graph, with edges weights encoding the similarity between nodes.

This paper illustrates the fundamental motivation for using a graph
representation: **smoothness**. In reality, we always face the curse of
dimensionality--the phenomenon that high-dimensional data is unavoidably
sparse. Graph smoothing is an important way to tackle this issue. Intuitively,
smoothing allows the model to infer an unseen data point by relating it to the
seen ones. The idea that nodes with connections share similarity is sometimes
also referred to as homophily, and is the key assumption in most current
research on Graph Neural Networks.

## Problem Formulation

We suppose there are $l$ labeled points $\left(x_1, y_1\right), \ldots,\left(x_l, y_l\right)$, and $u$ unlabeled points $x_{l+1}, \ldots, x_{l+u}$; typically $l \ll u$. Let $n=l+u$ be the total number of data points. To begin, we assume the labels are binary: $y \in\{0,1\}$. Consider a connected graph $G=(V, E)$ with nodes $V$ corresponding to the $n$ data points, with nodes $L=\{1, \ldots, l\}$ corresponding to the labeled points with labels $y_1, \ldots, y_l$, and nodes $U=\{l+1, \ldots, l+u\}$ corresponding to the unlabeled points. Our task is to assign labels to nodes $U$. We assume an $n \times n$ symmetric weight matrix $W$ on the edges of the graph is given. For example, when $x \in \mathbb{R}^m$, the weight matrix can be
\[
w_{i j}=\exp \left(-\sum_{d=1}^m \frac{\left(x_{i d}-x_{j d}\right)^2}{\sigma_d^2}\right)
\]
where $x_{i d}$ is the $d$-th component of instance $x_i$ represented as a vector $x_i \in \mathbb{R}^m$, and $\sigma_1, \ldots, \sigma_m$ are length scale hyperparameters for each dimension. Thus, nearby points in Euclidean space are assigned large edge weight. Other